{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    input_folder = 'combined_stats'\n",
    "    input_format = 'out'\n",
    "    feature = 'feature'\n",
    "    label = 'label'\n",
    "    X_train = np.loadtxt(\"{}/{}_{}.{}\".format(input_folder, feature, 'training', input_format))\n",
    "    y_train = np.loadtxt(\"{}/{}_{}.{}\".format(input_folder, label, 'training', input_format))\n",
    "    X_val = np.loadtxt(\"{}/{}_{}.{}\".format(input_folder, feature, 'validation', input_format))\n",
    "    y_val = np.loadtxt(\"{}/{}_{}.{}\".format(input_folder, label, 'validation', input_format))\n",
    "    X_test = np.loadtxt(\"{}/{}_{}.{}\".format(input_folder, feature, 'test', input_format))\n",
    "    y_test = np.loadtxt(\"{}/{}_{}.{}\".format(input_folder, label, 'test', input_format))\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1366, 76) (1366,) (106, 76) (106,) (59, 76) (59,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data()\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(76, 38)\n",
    "        self.fc2 = nn.Linear(38, 19)\n",
    "        self.fc3 = nn.Linear(19, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 76)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        y = self.out_act(x)\n",
    "        return y\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1366] loss: 4.793\n",
      "[2,  1366] loss: 4.660\n",
      "[3,  1366] loss: 4.644\n",
      "[4,  1366] loss: 4.628\n",
      "[5,  1366] loss: 4.615\n",
      "[6,  1366] loss: 4.602\n",
      "[7,  1366] loss: 4.590\n",
      "[8,  1366] loss: 4.579\n",
      "[9,  1366] loss: 4.569\n",
      "[10,  1366] loss: 4.559\n",
      "[11,  1366] loss: 4.550\n",
      "[12,  1366] loss: 4.542\n",
      "[13,  1366] loss: 4.533\n",
      "[14,  1366] loss: 4.526\n",
      "[15,  1366] loss: 4.519\n",
      "[16,  1366] loss: 4.512\n",
      "[17,  1366] loss: 4.506\n",
      "[18,  1366] loss: 4.500\n",
      "[19,  1366] loss: 4.494\n",
      "[20,  1366] loss: 4.489\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_training = X_train.shape[0]\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(n_training):\n",
    "        # get the inputs\n",
    "        inputs = X_train_tensor[i][:]\n",
    "        labels = y_train_tensor[i]\n",
    "        labels = labels.view(1, -1)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        # print(outputs, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % n_training == (n_training - 1):    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 64\n",
      "Accuracy of the network on the validation: 60 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "n_validation = X_val.shape[0]\n",
    "X_validation_tensor = torch.from_numpy(X_val).float()\n",
    "y_validation_tensor = torch.from_numpy(y_val).float()\n",
    "with torch.no_grad():\n",
    "    for i in range(n_validation):\n",
    "        inputs = X_validation_tensor[i][:]\n",
    "        labels = y_validation_tensor[i]\n",
    "        labels = labels.view(1, -1)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        if outputs.data >= 0.5:\n",
    "            predicted = 1\n",
    "        else:\n",
    "            predicted = 0\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).item()\n",
    "print(total, correct)\n",
    "print('Accuracy of the network on the validation: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1366] loss: 110.130\n",
      "[2,  1366] loss: 110.110\n",
      "[3,  1366] loss: 110.110\n",
      "[4,  1366] loss: 110.110\n",
      "[5,  1366] loss: 110.110\n",
      "[6,  1366] loss: 110.110\n",
      "[7,  1366] loss: 110.110\n",
      "[8,  1366] loss: 110.110\n",
      "[9,  1366] loss: 110.110\n",
      "[10,  1366] loss: 110.110\n",
      "[11,  1366] loss: 110.110\n",
      "[12,  1366] loss: 110.110\n",
      "[13,  1366] loss: 110.110\n",
      "[14,  1366] loss: 110.110\n",
      "[15,  1366] loss: 110.110\n",
      "[16,  1366] loss: 110.110\n",
      "[17,  1366] loss: 110.110\n",
      "[18,  1366] loss: 110.110\n",
      "[19,  1366] loss: 110.110\n",
      "[20,  1366] loss: 110.110\n",
      "[21,  1366] loss: 110.110\n",
      "[22,  1366] loss: 110.110\n",
      "[23,  1366] loss: 110.110\n",
      "[24,  1366] loss: 110.110\n",
      "[25,  1366] loss: 110.110\n",
      "[26,  1366] loss: 110.110\n",
      "[27,  1366] loss: 110.110\n",
      "[28,  1366] loss: 110.110\n",
      "[29,  1366] loss: 110.110\n",
      "[30,  1366] loss: 110.110\n",
      "[31,  1366] loss: 110.110\n",
      "[32,  1366] loss: 110.110\n",
      "[33,  1366] loss: 110.110\n",
      "[34,  1366] loss: 110.110\n",
      "[35,  1366] loss: 110.110\n",
      "[36,  1366] loss: 110.110\n",
      "[37,  1366] loss: 110.110\n",
      "[38,  1366] loss: 110.110\n",
      "[39,  1366] loss: 110.110\n",
      "[40,  1366] loss: 110.110\n",
      "[41,  1366] loss: 110.110\n",
      "[42,  1366] loss: 110.110\n",
      "[43,  1366] loss: 110.110\n",
      "[44,  1366] loss: 110.110\n",
      "[45,  1366] loss: 110.110\n",
      "[46,  1366] loss: 110.110\n",
      "[47,  1366] loss: 110.110\n",
      "[48,  1366] loss: 110.110\n",
      "[49,  1366] loss: 110.110\n",
      "[50,  1366] loss: 110.110\n",
      "Finished Training\n",
      "106 43\n",
      "Accuracy of the network on the validation: 40 %\n",
      "[1,  1366] loss: 4.802\n",
      "[2,  1366] loss: 4.664\n",
      "[3,  1366] loss: 4.656\n",
      "[4,  1366] loss: 4.652\n",
      "[5,  1366] loss: 4.651\n",
      "[6,  1366] loss: 4.651\n",
      "[7,  1366] loss: 4.651\n",
      "[8,  1366] loss: 4.651\n",
      "[9,  1366] loss: 4.651\n",
      "[10,  1366] loss: 4.651\n",
      "[11,  1366] loss: 4.651\n",
      "[12,  1366] loss: 4.651\n",
      "[13,  1366] loss: 4.651\n",
      "[14,  1366] loss: 4.651\n",
      "[15,  1366] loss: 4.651\n",
      "[16,  1366] loss: 4.651\n",
      "[17,  1366] loss: 4.651\n",
      "[18,  1366] loss: 4.651\n",
      "[19,  1366] loss: 4.651\n",
      "[20,  1366] loss: 4.651\n",
      "[21,  1366] loss: 4.651\n",
      "[22,  1366] loss: 4.651\n",
      "[23,  1366] loss: 4.651\n",
      "[24,  1366] loss: 4.651\n",
      "[25,  1366] loss: 4.651\n",
      "[26,  1366] loss: 4.651\n",
      "[27,  1366] loss: 4.651\n",
      "[28,  1366] loss: 4.651\n",
      "[29,  1366] loss: 4.651\n",
      "[30,  1366] loss: 4.651\n",
      "[31,  1366] loss: 4.651\n",
      "[32,  1366] loss: 4.651\n",
      "[33,  1366] loss: 4.651\n",
      "[34,  1366] loss: 4.651\n",
      "[35,  1366] loss: 4.651\n",
      "[36,  1366] loss: 4.651\n",
      "[37,  1366] loss: 4.651\n",
      "[38,  1366] loss: 4.651\n",
      "[39,  1366] loss: 4.651\n",
      "[40,  1366] loss: 4.651\n",
      "[41,  1366] loss: 4.651\n",
      "[42,  1366] loss: 4.651\n",
      "[43,  1366] loss: 4.651\n",
      "[44,  1366] loss: 4.651\n",
      "[45,  1366] loss: 4.651\n",
      "[46,  1366] loss: 4.651\n",
      "[47,  1366] loss: 4.651\n",
      "[48,  1366] loss: 4.651\n",
      "[49,  1366] loss: 4.651\n",
      "[50,  1366] loss: 4.651\n",
      "Finished Training\n",
      "106 63\n",
      "Accuracy of the network on the validation: 59 %\n",
      "[1,  1366] loss: 5.164\n",
      "[2,  1366] loss: 4.646\n",
      "[3,  1366] loss: 4.597\n",
      "[4,  1366] loss: 4.569\n",
      "[5,  1366] loss: 4.543\n",
      "[6,  1366] loss: 4.526\n",
      "[7,  1366] loss: 4.500\n",
      "[8,  1366] loss: 4.489\n",
      "[9,  1366] loss: 4.485\n",
      "[10,  1366] loss: 4.474\n",
      "[11,  1366] loss: 4.474\n",
      "[12,  1366] loss: 4.471\n",
      "[13,  1366] loss: 4.458\n",
      "[14,  1366] loss: 4.453\n",
      "[15,  1366] loss: 4.484\n",
      "[16,  1366] loss: 4.460\n",
      "[17,  1366] loss: 4.453\n",
      "[18,  1366] loss: 4.484\n",
      "[19,  1366] loss: 4.436\n",
      "[20,  1366] loss: 4.453\n",
      "[21,  1366] loss: 4.457\n",
      "[22,  1366] loss: 4.468\n",
      "[23,  1366] loss: 4.458\n",
      "[24,  1366] loss: 4.431\n",
      "[25,  1366] loss: 4.443\n",
      "[26,  1366] loss: 4.436\n",
      "[27,  1366] loss: 4.466\n",
      "[28,  1366] loss: 4.451\n",
      "[29,  1366] loss: 4.463\n",
      "[30,  1366] loss: 4.444\n",
      "[31,  1366] loss: 4.455\n",
      "[32,  1366] loss: 4.454\n",
      "[33,  1366] loss: 4.480\n",
      "[34,  1366] loss: 4.455\n",
      "[35,  1366] loss: 4.463\n",
      "[36,  1366] loss: 4.433\n",
      "[37,  1366] loss: 4.435\n",
      "[38,  1366] loss: 4.489\n",
      "[39,  1366] loss: 4.446\n",
      "[40,  1366] loss: 4.449\n",
      "[41,  1366] loss: 4.423\n",
      "[42,  1366] loss: 4.449\n",
      "[43,  1366] loss: 4.451\n",
      "[44,  1366] loss: 4.451\n",
      "[45,  1366] loss: 4.437\n",
      "[46,  1366] loss: 4.446\n",
      "[47,  1366] loss: 4.459\n",
      "[48,  1366] loss: 4.432\n",
      "[49,  1366] loss: 4.427\n",
      "[50,  1366] loss: 4.428\n",
      "Finished Training\n",
      "106 71\n",
      "Accuracy of the network on the validation: 66 %\n",
      "[1,  1366] loss: 4.654\n",
      "[2,  1366] loss: 4.589\n",
      "[3,  1366] loss: 4.554\n",
      "[4,  1366] loss: 4.527\n",
      "[5,  1366] loss: 4.511\n",
      "[6,  1366] loss: 4.504\n",
      "[7,  1366] loss: 4.486\n",
      "[8,  1366] loss: 4.484\n",
      "[9,  1366] loss: 4.475\n",
      "[10,  1366] loss: 4.465\n",
      "[11,  1366] loss: 4.468\n",
      "[12,  1366] loss: 4.461\n",
      "[13,  1366] loss: 4.456\n",
      "[14,  1366] loss: 4.450\n",
      "[15,  1366] loss: 4.449\n",
      "[16,  1366] loss: 4.445\n",
      "[17,  1366] loss: 4.443\n",
      "[18,  1366] loss: 4.437\n",
      "[19,  1366] loss: 4.430\n",
      "[20,  1366] loss: 4.433\n",
      "[21,  1366] loss: 4.425\n",
      "[22,  1366] loss: 4.429\n",
      "[23,  1366] loss: 4.434\n",
      "[24,  1366] loss: 4.425\n",
      "[25,  1366] loss: 4.420\n",
      "[26,  1366] loss: 4.418\n",
      "[27,  1366] loss: 4.417\n",
      "[28,  1366] loss: 4.410\n",
      "[29,  1366] loss: 4.408\n",
      "[30,  1366] loss: 4.406\n",
      "[31,  1366] loss: 4.405\n",
      "[32,  1366] loss: 4.437\n",
      "[33,  1366] loss: 4.414\n",
      "[34,  1366] loss: 4.401\n",
      "[35,  1366] loss: 4.400\n",
      "[36,  1366] loss: 4.389\n",
      "[37,  1366] loss: 4.388\n",
      "[38,  1366] loss: 4.387\n",
      "[39,  1366] loss: 4.390\n",
      "[40,  1366] loss: 4.382\n",
      "[41,  1366] loss: 4.400\n",
      "[42,  1366] loss: 4.383\n",
      "[43,  1366] loss: 4.378\n",
      "[44,  1366] loss: 4.386\n",
      "[45,  1366] loss: 4.372\n",
      "[46,  1366] loss: 4.380\n",
      "[47,  1366] loss: 4.376\n",
      "[48,  1366] loss: 4.376\n",
      "[49,  1366] loss: 4.371\n",
      "[50,  1366] loss: 4.369\n",
      "Finished Training\n",
      "106 66\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[1,  1366] loss: 4.623\n",
      "[2,  1366] loss: 4.561\n",
      "[3,  1366] loss: 4.542\n",
      "[4,  1366] loss: 4.524\n",
      "[5,  1366] loss: 4.514\n",
      "[6,  1366] loss: 4.505\n",
      "[7,  1366] loss: 4.497\n",
      "[8,  1366] loss: 4.489\n",
      "[9,  1366] loss: 4.482\n",
      "[10,  1366] loss: 4.475\n",
      "[11,  1366] loss: 4.469\n",
      "[12,  1366] loss: 4.463\n",
      "[13,  1366] loss: 4.458\n",
      "[14,  1366] loss: 4.452\n",
      "[15,  1366] loss: 4.447\n",
      "[16,  1366] loss: 4.442\n",
      "[17,  1366] loss: 4.438\n",
      "[18,  1366] loss: 4.433\n",
      "[19,  1366] loss: 4.427\n",
      "[20,  1366] loss: 4.423\n",
      "[21,  1366] loss: 4.420\n",
      "[22,  1366] loss: 4.418\n",
      "[23,  1366] loss: 4.415\n",
      "[24,  1366] loss: 4.412\n",
      "[25,  1366] loss: 4.411\n",
      "[26,  1366] loss: 4.409\n",
      "[27,  1366] loss: 4.407\n",
      "[28,  1366] loss: 4.407\n",
      "[29,  1366] loss: 4.404\n",
      "[30,  1366] loss: 4.404\n",
      "[31,  1366] loss: 4.401\n",
      "[32,  1366] loss: 4.400\n",
      "[33,  1366] loss: 4.398\n",
      "[34,  1366] loss: 4.396\n",
      "[35,  1366] loss: 4.397\n",
      "[36,  1366] loss: 4.395\n",
      "[37,  1366] loss: 4.394\n",
      "[38,  1366] loss: 4.394\n",
      "[39,  1366] loss: 4.392\n",
      "[40,  1366] loss: 4.391\n",
      "[41,  1366] loss: 4.391\n",
      "[42,  1366] loss: 4.390\n",
      "[43,  1366] loss: 4.389\n",
      "[44,  1366] loss: 4.389\n",
      "[45,  1366] loss: 4.388\n",
      "[46,  1366] loss: 4.387\n",
      "[47,  1366] loss: 4.387\n",
      "[48,  1366] loss: 4.387\n",
      "[49,  1366] loss: 4.385\n",
      "[50,  1366] loss: 4.384\n",
      "Finished Training\n",
      "106 65\n",
      "Accuracy of the network on the validation: 61 %\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "lrs = []\n",
    "for i in range(5):\n",
    "    lrs.append(pow(10, -(i + 1)))\n",
    "\n",
    "n_training = X_train.shape[0]\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "n_validation = X_val.shape[0]\n",
    "X_validation_tensor = torch.from_numpy(X_val).float()\n",
    "y_validation_tensor = torch.from_numpy(y_val).float()\n",
    "\n",
    "val_accuracy = []\n",
    "\n",
    "epoch_size = 50\n",
    "for learning_rate in lrs:\n",
    "    net = Net()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = learning_rate)\n",
    "    for epoch in range(epoch_size):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i in range(n_training):\n",
    "            # get the inputs\n",
    "            inputs = X_train_tensor[i][:]\n",
    "            labels = y_train_tensor[i]\n",
    "            labels = labels.view(1, -1)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            # print(outputs, labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % n_training == (n_training - 1):    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_validation):\n",
    "            inputs = X_validation_tensor[i][:]\n",
    "            labels = y_validation_tensor[i]\n",
    "            labels = labels.view(1, -1)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            if outputs.data >= 0.5:\n",
    "                predicted = 1\n",
    "            else:\n",
    "                predicted = 0\n",
    "            correct += (predicted == labels).item()\n",
    "    print(n_validation, correct)\n",
    "    print('Accuracy of the network on the validation: %d %%' % (\n",
    "        100 * correct / n_validation))\n",
    "    val_accuracy.append(correct / n_validation)\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.01, 0.001, 0.0001, 1e-05]\n",
      "[0.4056603773584906, 0.5943396226415094, 0.6698113207547169, 0.6226415094339622, 0.6132075471698113]\n"
     ]
    }
   ],
   "source": [
    "print(lrs)\n",
    "print(val_accuracy)\n",
    "# best lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1366] loss: 5.003\n",
      "Epoch 1 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 59 %\n",
      "[2,  1366] loss: 4.648\n",
      "Epoch 2 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 59 %\n",
      "[3,  1366] loss: 4.640\n",
      "Epoch 3 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 59 %\n",
      "[4,  1366] loss: 4.631\n",
      "Epoch 4 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 58 %\n",
      "[5,  1366] loss: 4.620\n",
      "Epoch 5 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 59 %\n",
      "[6,  1366] loss: 4.610\n",
      "Epoch 6 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[7,  1366] loss: 4.602\n",
      "Epoch 7 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[8,  1366] loss: 4.594\n",
      "Epoch 8 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[9,  1366] loss: 4.586\n",
      "Epoch 9 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[10,  1366] loss: 4.578\n",
      "Epoch 10 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[11,  1366] loss: 4.571\n",
      "Epoch 11 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[12,  1366] loss: 4.562\n",
      "Epoch 12 finished Training.\n",
      "Accuracy of the network on the training: 57 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[13,  1366] loss: 4.556\n",
      "Epoch 13 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 65 %\n",
      "[14,  1366] loss: 4.549\n",
      "Epoch 14 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[15,  1366] loss: 4.543\n",
      "Epoch 15 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[16,  1366] loss: 4.537\n",
      "Epoch 16 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[17,  1366] loss: 4.531\n",
      "Epoch 17 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[18,  1366] loss: 4.525\n",
      "Epoch 18 finished Training.\n",
      "Accuracy of the network on the training: 58 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[19,  1366] loss: 4.520\n",
      "Epoch 19 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[20,  1366] loss: 4.515\n",
      "Epoch 20 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[21,  1366] loss: 4.510\n",
      "Epoch 21 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[22,  1366] loss: 4.505\n",
      "Epoch 22 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[23,  1366] loss: 4.501\n",
      "Epoch 23 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[24,  1366] loss: 4.497\n",
      "Epoch 24 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[25,  1366] loss: 4.493\n",
      "Epoch 25 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[26,  1366] loss: 4.489\n",
      "Epoch 26 finished Training.\n",
      "Accuracy of the network on the training: 59 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[27,  1366] loss: 4.485\n",
      "Epoch 27 finished Training.\n",
      "Accuracy of the network on the training: 60 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[28,  1366] loss: 4.482\n",
      "Epoch 28 finished Training.\n",
      "Accuracy of the network on the training: 60 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[29,  1366] loss: 4.478\n",
      "Epoch 29 finished Training.\n",
      "Accuracy of the network on the training: 60 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[30,  1366] loss: 4.474\n",
      "Epoch 30 finished Training.\n",
      "Accuracy of the network on the training: 60 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[31,  1366] loss: 4.471\n",
      "Epoch 31 finished Training.\n",
      "Accuracy of the network on the training: 60 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[32,  1366] loss: 4.468\n",
      "Epoch 32 finished Training.\n",
      "Accuracy of the network on the training: 60 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[33,  1366] loss: 4.465\n",
      "Epoch 33 finished Training.\n",
      "Accuracy of the network on the training: 60 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[34,  1366] loss: 4.462\n",
      "Epoch 34 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[35,  1366] loss: 4.460\n",
      "Epoch 35 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[36,  1366] loss: 4.457\n",
      "Epoch 36 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[37,  1366] loss: 4.455\n",
      "Epoch 37 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[38,  1366] loss: 4.452\n",
      "Epoch 38 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[39,  1366] loss: 4.450\n",
      "Epoch 39 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[40,  1366] loss: 4.448\n",
      "Epoch 40 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[41,  1366] loss: 4.446\n",
      "Epoch 41 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[42,  1366] loss: 4.444\n",
      "Epoch 42 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[43,  1366] loss: 4.442\n",
      "Epoch 43 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[44,  1366] loss: 4.440\n",
      "Epoch 44 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[45,  1366] loss: 4.438\n",
      "Epoch 45 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[46,  1366] loss: 4.436\n",
      "Epoch 46 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[47,  1366] loss: 4.434\n",
      "Epoch 47 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[48,  1366] loss: 4.433\n",
      "Epoch 48 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[49,  1366] loss: 4.431\n",
      "Epoch 49 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[50,  1366] loss: 4.430\n",
      "Epoch 50 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[51,  1366] loss: 4.428\n",
      "Epoch 51 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[52,  1366] loss: 4.427\n",
      "Epoch 52 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[53,  1366] loss: 4.426\n",
      "Epoch 53 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[54,  1366] loss: 4.425\n",
      "Epoch 54 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[55,  1366] loss: 4.424\n",
      "Epoch 55 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[56,  1366] loss: 4.423\n",
      "Epoch 56 finished Training.\n",
      "Accuracy of the network on the training: 61 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[57,  1366] loss: 4.421\n",
      "Epoch 57 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58,  1366] loss: 4.420\n",
      "Epoch 58 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[59,  1366] loss: 4.419\n",
      "Epoch 59 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[60,  1366] loss: 4.418\n",
      "Epoch 60 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[61,  1366] loss: 4.417\n",
      "Epoch 61 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[62,  1366] loss: 4.417\n",
      "Epoch 62 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[63,  1366] loss: 4.416\n",
      "Epoch 63 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[64,  1366] loss: 4.415\n",
      "Epoch 64 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[65,  1366] loss: 4.414\n",
      "Epoch 65 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[66,  1366] loss: 4.413\n",
      "Epoch 66 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[67,  1366] loss: 4.413\n",
      "Epoch 67 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[68,  1366] loss: 4.412\n",
      "Epoch 68 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[69,  1366] loss: 4.411\n",
      "Epoch 69 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[70,  1366] loss: 4.410\n",
      "Epoch 70 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[71,  1366] loss: 4.410\n",
      "Epoch 71 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[72,  1366] loss: 4.409\n",
      "Epoch 72 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[73,  1366] loss: 4.408\n",
      "Epoch 73 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[74,  1366] loss: 4.408\n",
      "Epoch 74 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[75,  1366] loss: 4.407\n",
      "Epoch 75 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[76,  1366] loss: 4.406\n",
      "Epoch 76 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[77,  1366] loss: 4.406\n",
      "Epoch 77 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[78,  1366] loss: 4.405\n",
      "Epoch 78 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[79,  1366] loss: 4.405\n",
      "Epoch 79 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[80,  1366] loss: 4.404\n",
      "Epoch 80 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[81,  1366] loss: 4.404\n",
      "Epoch 81 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[82,  1366] loss: 4.403\n",
      "Epoch 82 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[83,  1366] loss: 4.403\n",
      "Epoch 83 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[84,  1366] loss: 4.402\n",
      "Epoch 84 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[85,  1366] loss: 4.402\n",
      "Epoch 85 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 60 %\n",
      "[86,  1366] loss: 4.401\n",
      "Epoch 86 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[87,  1366] loss: 4.400\n",
      "Epoch 87 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[88,  1366] loss: 4.400\n",
      "Epoch 88 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 61 %\n",
      "[89,  1366] loss: 4.399\n",
      "Epoch 89 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[90,  1366] loss: 4.399\n",
      "Epoch 90 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[91,  1366] loss: 4.399\n",
      "Epoch 91 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[92,  1366] loss: 4.398\n",
      "Epoch 92 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[93,  1366] loss: 4.398\n",
      "Epoch 93 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[94,  1366] loss: 4.397\n",
      "Epoch 94 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 62 %\n",
      "[95,  1366] loss: 4.397\n",
      "Epoch 95 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 63 %\n",
      "[96,  1366] loss: 4.396\n",
      "Epoch 96 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[97,  1366] loss: 4.395\n",
      "Epoch 97 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[98,  1366] loss: 4.393\n",
      "Epoch 98 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[99,  1366] loss: 4.393\n",
      "Epoch 99 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "[100,  1366] loss: 4.393\n",
      "Epoch 100 finished Training.\n",
      "Accuracy of the network on the training: 62 %\n",
      "Accuracy of the network on the validation: 64 %\n",
      "Traininig all done\n"
     ]
    }
   ],
   "source": [
    "n_training = X_train.shape[0]\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "n_validation = X_val.shape[0]\n",
    "X_validation_tensor = torch.from_numpy(X_val).float()\n",
    "y_validation_tensor = torch.from_numpy(y_val).float()\n",
    "n_test = X_test.shape[0]\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "epoches = []\n",
    "training_acc = []\n",
    "val_acc = []\n",
    "\n",
    "epoch_size = 100\n",
    "\n",
    "# set up the NN\n",
    "net = Net()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(epoch_size):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i in range(n_training):\n",
    "        # get the inputs\n",
    "        inputs = X_train_tensor[i][:]\n",
    "        labels = y_train_tensor[i]\n",
    "        labels = labels.view(1, -1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        # print(outputs, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % n_training == (n_training - 1):    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    print('Epoch %d finished Training.' %(epoch + 1))\n",
    "    \n",
    "    correct_training = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_training):\n",
    "            inputs = X_train_tensor[i][:]\n",
    "            labels = y_train_tensor[i]\n",
    "            labels = labels.view(1, -1)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            if outputs.data >= 0.5:\n",
    "                predicted = 1\n",
    "            else:\n",
    "                predicted = 0\n",
    "            correct_training += (predicted == labels).item()\n",
    "    print('Accuracy of the network on the training: %d %%' % (\n",
    "        100 * correct_training / n_training))\n",
    "    training_acc.append(correct_training / n_training)\n",
    "\n",
    "    correct_val = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_validation):\n",
    "            inputs = X_validation_tensor[i][:]\n",
    "            labels = y_validation_tensor[i]\n",
    "            labels = labels.view(1, -1)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            if outputs.data >= 0.5:\n",
    "                predicted = 1\n",
    "            else:\n",
    "                predicted = 0\n",
    "            correct_val += (predicted == labels).item()\n",
    "    print('Accuracy of the network on the validation: %d %%' % (\n",
    "        100 * correct_val / n_validation))\n",
    "    val_acc.append(correct_val / n_validation)\n",
    "    epoches.append(epoch + 1)\n",
    "print('Traininig all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFsYYY/yyysIYY4xf/w+Nzt3EjlKQRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a205379b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(epoches, training_acc, linewidth = 2, label = 'Train accuracy rate')\n",
    "plt.plot(epoches, val_acc, linewidth = 2, label='Validation accuracy rate')\n",
    "plt.title('Accuracy Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Rate')\n",
    "# plt.xlim([0, epoch_size])\n",
    "# plt.ylim([0, 1])\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.savefig('nn_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the validation: 64 %\n",
      "38 59\n"
     ]
    }
   ],
   "source": [
    "correct_test = 0\n",
    "y_pred = []\n",
    "y_pred1 = []\n",
    "y_true1 = []\n",
    "with torch.no_grad():\n",
    "    for i in range(n_test):\n",
    "        inputs = X_test_tensor[i][:]\n",
    "        labels = y_test_tensor[i]\n",
    "        labels = labels.view(1, -1)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        y_pred.append(outputs.data)\n",
    "        if outputs.data >= 0.5:\n",
    "            predicted = 1\n",
    "        else:\n",
    "            predicted = 0\n",
    "        correct_test += (predicted == labels).item()\n",
    "print('Accuracy of the network on the validation: %d %%' % (\n",
    "    100 * correct_test / n_test))\n",
    "print(correct_test, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7628915933898056\n"
     ]
    }
   ],
   "source": [
    "# calculate the AP score\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_pred = np.array(y_pred)\n",
    "ap = average_precision_score(y_test, y_pred)\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
